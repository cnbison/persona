"""
OpenAIå®¢æˆ·ç«¯å°è£…
æä¾›ç»Ÿä¸€çš„GPT-4è°ƒç”¨æ¥å£ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶ã€æµå¼å“åº”ã€æˆæœ¬ç»Ÿè®¡
"""
import asyncio
import time
from typing import Optional, Dict, Any, AsyncIterator
import httpx
from enum import Enum
from loguru import logger
import json

try:
    from openai import AsyncOpenAI, OpenAI
    from openai import Stream
    from openai.types import Completion
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("âš ï¸  OpenAIåŒ…æœªå®‰è£…ï¼Œå°†ä½¿ç”¨mockæ¨¡å¼")

from app.utils.config import settings


class ModelType(str, Enum):
    """æ”¯æŒçš„æ¨¡å‹ç±»å‹"""
    GPT4_TURBO = "gpt-4-turbo-preview"
    GPT4 = "gpt-4"
    GPT35_TURBO = "gpt-3.5-turbo"


class OpenAIClient:
    """
    OpenAIå®¢æˆ·ç«¯å°è£…ç±»

    åŠŸèƒ½ï¼š
    - ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
    - è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
    - æµå¼å“åº”æ”¯æŒ
    - Tokenä½¿ç”¨ç»Ÿè®¡
    - æˆæœ¬è®¡ç®—
    """

    # æ¨¡å‹å®šä»·ï¼ˆç¾å…ƒ/1K tokensï¼‰- 2025å¹´ä»·æ ¼
    PRICING = {
        "gpt-4-turbo-preview": {"input": 0.01, "output": 0.03},
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
    }

    def __init__(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        self.client = None
        self.async_client = None
        self.mock_mode = True
        self.active_provider_id = None
        self.active_provider = None
        self._init_from_provider()

    async def chat_completion(
        self,
        messages: list,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        max_retries: int = 3
    ) -> Dict[str, Any]:
        """
        èŠå¤©è¡¥å…¨APIï¼ˆå¼‚æ­¥ï¼‰

        å‚æ•°:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
                [{"role": "user", "content": "..."}, ...]
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®çš„æ¨¡å‹ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼‰
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¿”å›
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        è¿”å›:
            {
                "content": "å“åº”å†…å®¹",
                "usage": {"prompt_tokens": 10, "completion_tokens": 20},
                "model": "gpt-4-turbo-preview",
                "cost": 0.0007
            }
        """
        self._init_from_provider()

        if self.mock_mode:
            return self._mock_response(messages)

        provider_type = (self.active_provider or {}).get("provider_type", "openai")
        model = model or (self.active_provider or {}).get("model") or settings.openai_model
        temperature = temperature or settings.openai_temperature

        for attempt in range(max_retries):
            try:
                logger.debug(f"ğŸ”„ è°ƒç”¨OpenAI API (å°è¯• {attempt + 1}/{max_retries})")

                if provider_type in ("openai", "deepseek", "qwen", "ollama", "custom"):
                    response = await self.async_client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stream=stream
                    )

                    if stream:
                        content = ""
                        async for chunk in response:
                            if chunk.choices[0].delta.content:
                                content += chunk.choices[0].delta.content
                        completion_text = content
                        usage = None
                    else:
                        completion_text = response.choices[0].message.content
                        usage = response.usage
                elif provider_type == "azure":
                    completion_text, usage = await self._call_azure_chat(
                        messages=messages,
                        model=model,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                elif provider_type == "anthropic":
                    completion_text, usage = await self._call_anthropic(
                        messages=messages,
                        model=model,
                        max_tokens=max_tokens
                    )
                else:
                    raise RuntimeError(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›æ–¹: {provider_type}")

                # è®¡ç®—æˆæœ¬
                cost = self._calculate_cost(model, usage) if usage else 0.0

                # è®°å½•ä½¿ç”¨æƒ…å†µ
                if usage:
                    logger.info(
                        f"âœ… OpenAIè°ƒç”¨æˆåŠŸ | "
                        f"è¾“å…¥: {usage.prompt_tokens} tokens | "
                        f"è¾“å‡º: {usage.completion_tokens} tokens | "
                        f"æˆæœ¬: ${cost:.4f}"
                    )

                return {
                    "content": completion_text,
                    "usage": {
                        "prompt_tokens": usage.prompt_tokens if usage else 0,
                        "completion_tokens": usage.completion_tokens if usage else 0,
                        "total_tokens": usage.total_tokens if usage else 0
                    } if usage else None,
                    "model": model,
                    "cost": cost
                }

            except Exception as e:
                logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")

                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise

                # æŒ‡æ•°é€€é¿
                wait_time = 2 ** attempt
                logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)

    def chat_completion_sync(
        self,
        messages: list,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        max_retries: int = 3
    ) -> Dict[str, Any]:
        """
        èŠå¤©è¡¥å…¨APIï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

        å‚æ•°å’Œè¿”å›å€¼ä¸å¼‚æ­¥ç‰ˆæœ¬ç›¸åŒ
        """
        self._init_from_provider()

        if self.mock_mode:
            return self._mock_response(messages)

        provider_type = (self.active_provider or {}).get("provider_type", "openai")
        model = model or (self.active_provider or {}).get("model") or settings.openai_model
        temperature = temperature or settings.openai_temperature

        for attempt in range(max_retries):
            try:
                logger.debug(f"ğŸ”„ è°ƒç”¨OpenAI API (å°è¯• {attempt + 1}/{max_retries})")

                if provider_type in ("openai", "deepseek", "qwen", "ollama", "custom"):
                    response = self.client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )

                    completion_text = response.choices[0].message.content
                    usage = response.usage
                elif provider_type == "azure":
                    completion_text, usage = asyncio.run(
                        self._call_azure_chat(messages, model, temperature, max_tokens)
                    )
                elif provider_type == "anthropic":
                    completion_text, usage = asyncio.run(
                        self._call_anthropic(messages, model, max_tokens)
                    )
                else:
                    raise RuntimeError(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›æ–¹: {provider_type}")

                cost = self._calculate_cost(model, usage) if usage else 0.0

                logger.info(
                    f"âœ… OpenAIè°ƒç”¨æˆåŠŸ | "
                    f"è¾“å…¥: {usage.prompt_tokens} tokens | "
                    f"è¾“å‡º: {usage.completion_tokens} tokens | "
                    f"æˆæœ¬: ${cost:.4f}"
                )

                return {
                    "content": completion_text,
                    "usage": {
                        "prompt_tokens": usage.prompt_tokens if usage else 0,
                        "completion_tokens": usage.completion_tokens if usage else 0,
                        "total_tokens": usage.total_tokens if usage else 0
                    } if usage else None,
                    "model": model,
                    "cost": cost
                }

            except Exception as e:
                logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")

                if attempt == max_retries - 1:
                    raise

                wait_time = 2 ** attempt
                logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)

    def _calculate_cost(self, model: str, usage) -> float:
        """è®¡ç®—APIè°ƒç”¨æˆæœ¬"""
        if not usage:
            return 0.0

        pricing = self.PRICING.get(model, {"input": 0.01, "output": 0.03})
        input_cost = (usage.prompt_tokens / 1000) * pricing["input"]
        output_cost = (usage.completion_tokens / 1000) * pricing["output"]

        return input_cost + output_cost

    def _init_from_provider(self):
        if not OPENAI_AVAILABLE:
            self.mock_mode = True
            return

        provider = self._load_active_provider()
        if not provider:
            if settings.openai_api_key == "sk-test-key":
                self.mock_mode = True
                return
            provider = {
                "provider_id": None,
                "provider_type": "openai",
                "api_key": settings.openai_api_key,
                "base_url": settings.openai_api_base,
                "model": settings.openai_model,
            }

        if provider.get("provider_id") == self.active_provider_id:
            return

        self.active_provider_id = provider.get("provider_id")
        self.active_provider = provider

        api_key = provider.get("api_key") or ""
        base_url = provider.get("base_url") or settings.openai_api_base

        if not api_key or api_key == "sk-test-key":
            self.mock_mode = True
            self.client = None
            self.async_client = None
            return

        try:
            self.client = OpenAI(api_key=api_key, base_url=base_url)
            self.async_client = AsyncOpenAI(api_key=api_key, base_url=base_url)
            self.mock_mode = False
            logger.info(f"âœ… æ¨¡å‹æä¾›æ–¹åŠ è½½æˆåŠŸ: {provider.get('name', provider.get('provider_type'))}")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹æä¾›æ–¹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.mock_mode = True
            self.client = None
            self.async_client = None

    def _load_active_provider(self) -> Optional[Dict[str, Any]]:
        try:
            from app.database import SessionLocal
            from app.models.orm import ModelProviderORM

            db = SessionLocal()
            provider = db.query(ModelProviderORM).filter(ModelProviderORM.is_active == 1).first()
            db.close()

            if not provider:
                return None

            return {
                "provider_id": provider.provider_id,
                "name": provider.name,
                "provider_type": provider.provider_type,
                "base_url": provider.base_url,
                "api_key": provider.api_key,
                "api_version": provider.api_version,
                "model": provider.model,
                "extra_headers": provider.extra_headers or {}
            }
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹æä¾›æ–¹å¤±è´¥: {e}")
            return None

    async def _call_azure_chat(self, messages, model, temperature, max_tokens):
        provider = self.active_provider or {}
        base_url = provider.get("base_url") or ""
        api_key = provider.get("api_key") or ""
        api_version = provider.get("api_version") or "2024-02-15-preview"

        if not base_url or not api_key:
            raise RuntimeError("Azureé…ç½®ç¼ºå¤± base_url æˆ– api_key")

        url = f"{base_url}/openai/deployments/{model}/chat/completions?api-version={api_version}"
        payload = {
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        headers = {"api-key": api_key, "Content-Type": "application/json"}
        headers.update(provider.get("extra_headers", {}))

        async with httpx.AsyncClient(timeout=60) as client:
            resp = await client.post(url, json=payload, headers=headers)
            resp.raise_for_status()
            data = resp.json()

        completion_text = data["choices"][0]["message"]["content"]
        usage = data.get("usage")
        return completion_text, usage

    async def _call_anthropic(self, messages, model, max_tokens):
        provider = self.active_provider or {}
        base_url = provider.get("base_url") or "https://api.anthropic.com"
        api_key = provider.get("api_key") or ""
        api_version = provider.get("api_version") or "2023-06-01"

        if not api_key:
            raise RuntimeError("Anthropicé…ç½®ç¼ºå¤± api_key")

        system_parts = [m["content"] for m in messages if m["role"] == "system"]
        user_messages = [m for m in messages if m["role"] != "system"]
        if system_parts and user_messages:
            user_messages[0]["content"] = "System: " + " ".join(system_parts) + "\n" + user_messages[0]["content"]

        url = f"{base_url}/v1/messages"
        payload = {
            "model": model,
            "max_tokens": max_tokens or 1024,
            "messages": user_messages
        }
        headers = {
            "x-api-key": api_key,
            "anthropic-version": api_version,
            "content-type": "application/json"
        }
        headers.update(provider.get("extra_headers", {}))

        async with httpx.AsyncClient(timeout=60) as client:
            resp = await client.post(url, json=payload, headers=headers)
            resp.raise_for_status()
            data = resp.json()

        content_blocks = data.get("content", [])
        completion_text = content_blocks[0].get("text", "") if content_blocks else ""
        usage = data.get("usage")
        return completion_text, usage

    def _mock_response(self, messages: list) -> Dict[str, Any]:
        """
        Mockå“åº”ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰
        """
        last_message = messages[-1]["content"] if messages else ""

        mock_content = f"[Mockå“åº”] è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„OpenAIå“åº”ã€‚\n\nä½ çš„è¾“å…¥æ˜¯ï¼š{last_message[:100]}...\n\né…ç½®çœŸå®APIå¯†é’¥åå³å¯è°ƒç”¨å®é™…APIã€‚"

        return {
            "content": mock_content,
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 20,
                "total_tokens": 30
            },
            "model": settings.openai_model,
            "cost": 0.0
        }


# å…¨å±€å•ä¾‹
_openai_client: Optional[OpenAIClient] = None


def get_openai_client() -> OpenAIClient:
    """
    è·å–OpenAIå®¢æˆ·ç«¯å•ä¾‹

    ä½¿ç”¨æ–¹å¼:
        client = get_openai_client()
        response = await client.chat_completion(messages)
    """
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAIClient()
    return _openai_client


# ä¾¿æ·å‡½æ•°
async def call_openai(
    messages: list,
    model: Optional[str] = None,
    temperature: Optional[float] = None
) -> str:
    """
    ä¾¿æ·çš„OpenAIè°ƒç”¨å‡½æ•°ï¼ˆåªè¿”å›å†…å®¹ï¼‰

    ä½¿ç”¨æ–¹å¼:
        response = await call_openai([{"role": "user", "content": "ä½ å¥½"}])
    """
    client = get_openai_client()
    result = await client.chat_completion(
        messages=messages,
        model=model,
        temperature=temperature
    )
    return result["content"]


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import asyncio

    async def test():
        """æµ‹è¯•OpenAIå®¢æˆ·ç«¯"""
        client = get_openai_client()

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€ã€‚"}
        ]

        try:
            response = await client.chat_completion(messages)
            print("å“åº”å†…å®¹:")
            print(response["content"])
            print(f"\nä½¿ç”¨æƒ…å†µ: {response['usage']}")
            print(f"æˆæœ¬: ${response['cost']:.4f}")
        except Exception as e:
            print(f"é”™è¯¯: {e}")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test())
