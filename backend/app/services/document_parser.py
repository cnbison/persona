"""
æ–‡æ¡£è§£ææœåŠ¡
è´Ÿè´£è§£æå„ç§æ ¼å¼çš„è‘—ä½œæ–‡ä»¶ï¼Œæå–ç»“æ„åŒ–å†…å®¹
"""
import re
from collections import Counter
from pathlib import Path
from typing import Optional, List, Dict
from loguru import logger
import uuid

from app.models.book import Book, Chapter, CoreViewpoint
from app.utils.text_processor import get_text_processor
from app.utils.file_handler import get_file_handler

# å°è¯•å¯¼å…¥æ–‡æ¡£è§£æåº“
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    logger.warning("âš ï¸  pdfplumberæœªå®‰è£…ï¼ŒPDFè§£æåŠŸèƒ½ä¸å¯ç”¨")

try:
    import ebooklib
    from ebooklib import epub
    EBOOKLIB_AVAILABLE = True
except ImportError:
    EBOOKLIB_AVAILABLE = False
    logger.warning("âš ï¸  ebooklibæœªå®‰è£…ï¼ŒEPUBè§£æåŠŸèƒ½ä¸å¯ç”¨")

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    logger.warning("âš ï¸  python-docxæœªå®‰è£…ï¼ŒDOCXè§£æåŠŸèƒ½ä¸å¯ç”¨")


class DocumentParser:
    """
    æ–‡æ¡£è§£ææœåŠ¡

    åŠŸèƒ½ï¼š
    - è§£æPDFã€EPUBã€TXTã€DOCXæ ¼å¼
    - è¯†åˆ«ç« èŠ‚ç»“æ„
    - æå–æ ¸å¿ƒè§‚ç‚¹
    - ç»“æ„åŒ–å­˜å‚¨
    """

    def __init__(self):
        """åˆå§‹åŒ–æ–‡æ¡£è§£æå™¨"""
        self.text_processor = get_text_processor()
        self.file_handler = get_file_handler()
        logger.info("âœ… æ–‡æ¡£è§£ææœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

    async def parse_book(
        self,
        file_path: str,
        title: Optional[str] = None,
        author: Optional[str] = None
    ) -> Book:
        """
        è§£æè‘—ä½œæ–‡ä»¶

        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„
            title: è‘—ä½œæ ‡é¢˜ï¼ˆå¯é€‰ï¼Œä»æ–‡ä»¶åæå–ï¼‰
            author: ä½œè€…ï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            Bookå¯¹è±¡
        """
        logger.info(f"ğŸ“– å¼€å§‹è§£æè‘—ä½œ: {file_path}")

        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_info = self.file_handler.get_file_info(file_path)
        if not file_info:
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # ç¡®å®šæ–‡ä»¶ç±»å‹
        file_ext = file_info['extension']

        # æå–æ–‡æœ¬å†…å®¹
        if file_ext == 'pdf':
            text = await self._parse_pdf(file_path)
        elif file_ext == 'epub':
            text = await self._parse_epub(file_path)
        elif file_ext == 'txt':
            text = await self._parse_txt(file_path)
        elif file_ext == 'docx':
            text = await self._parse_docx(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")

        raw_text = text
        logger.info(f"âœ… æ–‡æœ¬æå–å®Œæˆï¼Œæ€»å­—æ•°: {len(text)}")

        # æ¸…æ´—æ–‡æœ¬
        logger.info("ğŸ§¹ å¼€å§‹æ¸…æ´—æ–‡æœ¬...")
        text = self.text_processor.clean_text(text)
        text = self.text_processor.remove_redundant_info(text)
        text = self._remove_repeated_lines(text)
        cleaned_lines = [line for line in text.split('\n') if line.strip()]
        raw_lines = [line for line in raw_text.split('\n') if line.strip()]
        logger.info(f"âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆï¼Œæ¸…æ´—åå­—æ•°: {len(text)}")

        # è¯†åˆ«ç« èŠ‚
        logger.info("ğŸ“š å¼€å§‹è¯†åˆ«ç« èŠ‚ç»“æ„...")
        chapters, chapter_stats = self._identify_chapters(text, file_ext, title or Path(file_path).stem)
        logger.info(f"âœ… è¯†åˆ«åˆ° {len(chapters)} ä¸ªç« èŠ‚")

        # æå–æ ¸å¿ƒè§‚ç‚¹
        logger.info("ğŸ’¡ å¼€å§‹æå–æ ¸å¿ƒè§‚ç‚¹...")
        core_viewpoints = await self._extract_core_viewpoints(chapters)
        logger.info(f"âœ… æå–åˆ° {len(core_viewpoints)} ä¸ªæ ¸å¿ƒè§‚ç‚¹")

        # åˆ›å»ºBookå¯¹è±¡
        parse_stats = {
            "raw_chars": len(raw_text),
            "cleaned_chars": len(text),
            "raw_lines": len(raw_lines),
            "cleaned_lines": len(cleaned_lines),
            "chapters_detected": len(chapters),
            "chapter_detection": chapter_stats
        }

        book = Book(
            book_id=str(uuid.uuid4()),
            title=title or Path(file_path).stem,
            author=author or "æœªçŸ¥ä½œè€…",
            language=self.text_processor.detect_language(text),
            file_path=file_path,
            file_type=file_ext,
            chapters=chapters,
            core_viewpoints=core_viewpoints,
            total_words=self.text_processor.count_words(text),
            parse_stats=parse_stats
        )

        logger.info(f"ğŸ‰ è‘—ä½œè§£æå®Œæˆ: {book.title}")
        return book

    async def _parse_pdf(self, file_path: str) -> str:
        """è§£æPDFæ–‡ä»¶"""
        if not PDFPLUMBER_AVAILABLE:
            raise ImportError("pdfplumberæœªå®‰è£…ï¼Œæ— æ³•è§£æPDFæ–‡ä»¶")

        text = ""
        try:
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n\n"

                    if page_num % 10 == 0:
                        logger.debug(f"  å·²å¤„ç† {page_num + 1}/{len(pdf.pages)} é¡µ")

        except Exception as e:
            logger.error(f"âŒ PDFè§£æå¤±è´¥: {e}")
            raise

        return text

    async def _parse_epub(self, file_path: str) -> str:
        """è§£æEPUBæ–‡ä»¶"""
        if not EBOOKLIB_AVAILABLE:
            raise ImportError("ebooklibæœªå®‰è£…ï¼Œæ— æ³•è§£æEPUBæ–‡ä»¶")

        try:
            book = epub.read_epub(file_path)
            text = ""

            # è·å–æ‰€æœ‰ç« èŠ‚
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    # æå–HTMLå†…å®¹
                    content = item.get_content()
                    # ç®€å•çš„HTMLæ ‡ç­¾å»é™¤
                    content = re.sub(r'<[^>]+>', '\n', content.decode('utf-8'))
                    text += content + "\n\n"

        except Exception as e:
            logger.error(f"âŒ EPUBè§£æå¤±è´¥: {e}")
            raise

        return text

    async def _parse_txt(self, file_path: str) -> str:
        """è§£æTXTæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    text = f.read()
            except:
                raise ValueError("æ— æ³•è§£ç æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶ç¼–ç ä¸ºUTF-8æˆ–GBK")
        except Exception as e:
            logger.error(f"âŒ TXTè§£æå¤±è´¥: {e}")
            raise

        return text

    async def _parse_docx(self, file_path: str) -> str:
        """è§£æDOCXæ–‡ä»¶"""
        if not DOCX_AVAILABLE:
            raise ImportError("python-docxæœªå®‰è£…ï¼Œæ— æ³•è§£æDOCXæ–‡ä»¶")

        try:
            doc = docx.Document(file_path)
            paragraphs = []
            for paragraph in doc.paragraphs:
                content = paragraph.text.strip()
                if content:
                    paragraphs.append(content)
            return "\n\n".join(paragraphs)
        except Exception as e:
            logger.error(f"âŒ DOCXè§£æå¤±è´¥: {e}")
            raise

    def _remove_repeated_lines(self, text: str) -> str:
        """ç§»é™¤é‡å¤é¡µçœ‰é¡µè„šç­‰å™ªéŸ³è¡Œ"""
        if not text:
            return text

        lines = [line.strip() for line in text.split('\n')]
        total = len(lines)
        counts = Counter([line for line in lines if line])

        def is_noise(line: str) -> bool:
            if not line:
                return False
            if re.match(r'^\d+$', line):
                return True
            if re.match(r'^-?\s*\d+\s*-?$', line):
                return True
            if re.match(r'^ç¬¬?\s*\d+\s*é¡µ$', line):
                return True
            if re.match(r'^[IVXLCM]+$', line):
                return True
            if re.match(r'^[\W_]+$', line):
                return True
            return False

        filtered = []
        for line in lines:
            if not line:
                filtered.append("")
                continue
            if is_noise(line):
                continue
            if len(line) <= 30 and counts[line] >= 3 and (counts[line] / max(total, 1)) >= 0.05:
                continue
            filtered.append(line)

        return "\n".join(filtered).strip()

    def _identify_chapters(self, text: str, file_type: str, book_title: Optional[str] = None) -> tuple[List[Chapter], Dict]:
        """
        è¯†åˆ«ç« èŠ‚ç»“æ„

        ç­–ç•¥ï¼š
        1. æŸ¥æ‰¾å¸¸è§çš„ç« èŠ‚æ ‡é¢˜æ¨¡å¼
        2. å°è¯•åŒ¹é…å·²çŸ¥ä¹¦ç±çš„ç« èŠ‚åˆ—è¡¨ï¼ˆå¦‚ã€Šä¹¡åœŸä¸­å›½ã€‹ï¼‰
        3. æŒ‰ç« èŠ‚åˆ†å‰²æ–‡æœ¬
        4. ä¸ºæ¯ä¸ªç« èŠ‚åˆ›å»ºChapterå¯¹è±¡
        """
        chapters = []
        stats = {
            "strategy": "pattern",
            "patterns_matched": 0,
            "known_book_hit": None,
            "fallback": False
        }

        # å·²çŸ¥ä¹¦ç±çš„ç« èŠ‚åˆ—è¡¨
        known_book_chapters = {
            'ä¹¡åœŸä¸­å›½': [
                'ä¹¡åœŸæœ¬è‰²',
                'æ–‡å­—ä¸‹ä¹¡',
                'å†è®ºæ–‡å­—ä¸‹ä¹¡',
                'å·®åºæ ¼å±€',
                'ç³»ç»´ç€ç§äººçš„é“å¾·',
                'å®¶æ—',
                'ç”·å¥³æœ‰åˆ«',
                'ç¤¼æ²»ç§©åº',
                'æ— è®¼',
                'æ— ä¸ºæ”¿æ²»',
                'é•¿è€ç»Ÿæ²»',
                'è¡€ç¼˜å’Œåœ°ç¼˜',
                'åå®çš„åˆ†ç¦»',
                'ä»æ¬²æœ›åˆ°éœ€è¦'
            ],
            'è®ºè¯­': [
                'å­¦è€Œ',
                'ä¸ºæ”¿',
                'å…«ä½¾',
                'é‡Œä»',
                'å…¬å†¶é•¿',
                'é›ä¹Ÿ',
                'è¿°è€Œ',
                'æ³°ä¼¯',
                'å­ç½•',
                'ä¹¡å…š',
                'å…ˆè¿›',
                'é¢œæ¸Š',
                'å­è·¯',
                'å®ªé—®',
                'å«çµå…¬',
                'å­£æ°',
                'é˜³è´§',
                'å¾®å­',
                'å­å¼ ',
                'å°§æ›°'
            ]
        }

        chapter_patterns = [
            r'^ç¬¬ã€\d+ã€‘æ®µ.*',  # "ç¬¬ã€Xã€‘æ®µï¼šYå·"ï¼ˆç†æƒ³å›½æ ¼å¼ï¼‰
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+ç« \s*',  # "ç¬¬Xç« "
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+å·\s*',  # "ç¬¬Xå·"
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+ç¯‡\s*',  # "ç¬¬Xç¯‡"ï¼ˆè®ºè¯­æ ¼å¼ï¼‰
            r'^[\u4e00-\u9fff]{1,3}ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+[å·ç¯‡ç« æœŸ]\s*',  # "å­¦è€Œç¬¬ä¸€å·"ã€"ä¸ºæ”¿ç¬¬äºŒç¯‡"ï¼ˆå¿…é¡»ä»¥å·/ç¯‡/ç« /æœŸç»“å°¾ï¼‰
            r'^[\u4e00-\u9fff]{1,3}ç¬¬\d+[å·ç¯‡ç« æœŸ]\s*',  # "å…¬å†¶é•¿ç¬¬äº”å·"ã€"å…ˆè¿›ç¬¬åä¸€ç¯‡"ï¼ˆå¿…é¡»ä»¥å·/ç¯‡/ç« /æœŸç»“å°¾ï¼‰
            r'^Chapter\s*\d+',  # "Chapter X"
            r'^(Chapter|CHAPTER)\s+[IVXLC]+',  # "CHAPTER IV"
            r'^(Part|PART)\s+\d+',  # "Part 1"
            r'^(Part|PART)\s+[IVXLC]+',  # "PART II"
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+èŠ‚\s*',  # "ç¬¬XèŠ‚"
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+\.\s',  # "ä¸€. "ã€"1. "
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶]+ã€\s',  # "ä¸€ã€"ã€"äºŒã€"
        ]

        def normalize_title(raw: str) -> str:
            if not raw:
                return ""
            cleaned = re.sub(r'[\s\u3000]+', '', raw)
            cleaned = re.sub(r'[Â·â€¢\-\â€”\â€•\â€“\:\ï¼š\.\ã€‚]', '', cleaned)
            cleaned = re.sub(r'[ç« èŠ‚å·ç¯‡]', '', cleaned)
            cleaned = cleaned.replace('ç¬¬', '')
            return cleaned

        # æŸ¥æ‰¾æ‰€æœ‰ç« èŠ‚æ ‡é¢˜ä½ç½®
        chapter_positions = []
        lines = text.split('\n')

        def is_noise_line(line: str) -> bool:
            if not line:
                return True
            if re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\[\d]]', line):
                return True
            if re.match(r'^\d+$', line):
                return True
            if re.match(r'^-?\s*\d+\s*-?$', line):
                return True
            if re.match(r'^ç¬¬?\s*\d+\s*é¡µ$', line):
                return True
            return False

        # ç­–ç•¥1: é€šç”¨ç« èŠ‚æ ‡é¢˜è¯„åˆ†
        for i, line in enumerate(lines):
            stripped_line = line.strip()
            if is_noise_line(stripped_line):
                continue

            score = 0
            # è¡Œé•¿åº¦æ›´åƒæ ‡é¢˜ï¼ˆçŸ­è¡Œï¼‰
            if 1 <= len(stripped_line) <= 20:
                score += 2
            elif len(stripped_line) <= 30:
                score += 1

            # å¸¸è§æ ‡é¢˜æ¨¡å¼
            for pattern in chapter_patterns:
                if re.match(pattern, stripped_line):
                    score += 3
                    break

            # ç« èŠ‚å…³é”®è¯
            if re.search(r'(ç« |å·|ç¯‡|èŠ‚|Chapter|CHAPTER|Part|PART)', stripped_line):
                score += 2

            # å‰åç©ºè¡Œï¼ˆæ ‡é¢˜å¸¸ç‹¬å è¡Œï¼‰
            prev_line = lines[i - 1].strip() if i > 0 else ""
            next_line = lines[i + 1].strip() if i + 1 < len(lines) else ""
            if not prev_line:
                score += 1
            if not next_line:
                score += 1

            if score >= 4:
                title = stripped_line
                if 'ç¬¬ã€' in title and 'æ®µï¼š' in title:
                    parts = title.split('ï¼š', 1)
                    if len(parts) > 1:
                        title = parts[1].strip()
                chapter_positions.append({
                    'line_num': i,
                    'title': title,
                    'content_start': i + 1
                })
                stats["patterns_matched"] += 1

        # ç­–ç•¥2: å°è¯•åŒ¹é…å·²çŸ¥ä¹¦ç±çš„ç« èŠ‚åˆ—è¡¨ï¼ˆé€šç”¨è¾…åŠ©ï¼‰
        for book_name, chapters_list in known_book_chapters.items():
            found_chapters = []
            normalized_targets = {normalize_title(t): t for t in chapters_list}

            for i, line in enumerate(lines):
                stripped = line.strip()
                if not stripped:
                    continue
                normalized_line = normalize_title(stripped)
                if normalized_line in normalized_targets:
                    found_chapters.append({
                        'line_num': i,
                        'title': normalized_targets[normalized_line],
                        'content_start': i + 1
                    })
                    continue

                # å¤„ç†â€œå­¦è€Œç¬¬ä¸€/ä¸ºæ”¿ç¬¬äºŒâ€ç­‰æ ·å¼ï¼šå‰ç¼€åŒ¹é…ç« èŠ‚å
                for normalized_target, original_title in normalized_targets.items():
                    if normalized_line.startswith(normalized_target) and len(normalized_line) <= len(normalized_target) + 4:
                        found_chapters.append({
                            'line_num': i,
                            'title': original_title,
                            'content_start': i + 1
                        })
                        break

            # å¦‚æœæ‰¾åˆ°è‡³å°‘ä¸€åŠç« èŠ‚ï¼Œè®¤ä¸ºåŒ¹é…æˆåŠŸ
            threshold = max(3, len(chapters_list) // 2)
            if len(found_chapters) >= threshold:
                found_chapters.sort(key=lambda x: x['line_num'])
                chapter_positions = found_chapters
                stats["strategy"] = "known_book"
                stats["known_book_hit"] = book_name
                logger.info(f"âœ… è¯†åˆ«ä¸ºã€Š{book_name}ã€‹æ ¼å¼ï¼Œæ‰¾åˆ° {len(found_chapters)} ä¸ªç« èŠ‚")
                break

        # å¦‚æœæ²¡æ‰¾åˆ°ç« èŠ‚ï¼ŒæŒ‰æ®µè½åˆ†å‰²
        if not chapter_positions:
            logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°ç« èŠ‚ç»“æ„ï¼ŒæŒ‰æ®µè½åˆ†å‰²")
            stats["strategy"] = "fallback_paragraph"
            stats["fallback"] = True
            paragraphs = self.text_processor.split_text_by_paragraph(text)

            # æ¯10ä¸ªæ®µè½åˆå¹¶ä¸ºä¸€ç« 
            chunk_size = 10
            for i in range(0, len(paragraphs), chunk_size):
                chapter_text = '\n\n'.join(paragraphs[i:i + chunk_size])
                chapters.append(Chapter(
                    chapter_id=str(uuid.uuid4()),
                    chapter_number=len(chapters) + 1,
                    title=f"æ®µè½ {i // chunk_size + 1}",
                    content=chapter_text,
                    page_range=None
                ))
        else:
            # æ™ºèƒ½åˆå¹¶ï¼šæŒ‰æ ‡é¢˜åˆ†ç»„ï¼ˆå¦‚å¤šæ®µå±äºåŒä¸€å·åˆ™åˆå¹¶ï¼‰
            logger.info(f"ğŸ“š æ£€æµ‹åˆ° {len(chapter_positions)} ä¸ªç« èŠ‚æ ‡è®°ï¼Œå¼€å§‹æ™ºèƒ½åˆå¹¶...")

            # æå–ä¸»è¦çš„ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚"ç¬¬ä¸€å·"ã€"ç¬¬äºŒå·"ï¼‰
            major_chapters = []
            current_content_lines = []
            current_major_title = None

            for i, pos in enumerate(chapter_positions):
                title = pos['title']
                start_line = pos['content_start']

                # åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¦ç« èŠ‚æ ‡é¢˜
                is_major = (
                    'å·' in title or
                    'ç« ' in title or
                    'ç¯‡' in title or
                    'Chapter' in title.lower() or
                    re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶]+ã€', title) or
                    re.match(r'^[\u4e00-\u9fff]{1,3}ç¬¬[\u4e00-\u9fffä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+', title) or  # "å­¦è€Œç¬¬ä¸€"
                    (re.search(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+$', title) and len(title) <= 5)  # "å…¬å†¶é•¿ç¬¬äº”"
                )

                if is_major:
                    # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚
                    if current_major_title and current_content_lines:
                        chapter_text = '\n'.join(current_content_lines).strip()
                        if chapter_text:
                            chapters.append(Chapter(
                                chapter_id=str(uuid.uuid4()),
                                chapter_number=len(chapters) + 1,
                                title=current_major_title,
                                content=chapter_text,
                                page_range=None
                            ))

                    # å¼€å§‹æ–°ç« èŠ‚
                    current_major_title = title
                    current_content_lines = []
                else:
                    # ä¸æ˜¯ä¸»è¦æ ‡é¢˜ï¼Œç»§ç»­ç´¯ç§¯å†…å®¹
                    pass

                # ç¡®å®šå†…å®¹èŒƒå›´
                if i < len(chapter_positions) - 1:
                    end_line = chapter_positions[i + 1]['line_num']
                else:
                    end_line = len(lines)

                # ç´¯ç§¯å†…å®¹è¡Œ
                content_lines = lines[start_line:end_line]
                current_content_lines.extend(content_lines)

            # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
            if current_major_title and current_content_lines:
                chapter_text = '\n'.join(current_content_lines).strip()
                if chapter_text:
                    chapters.append(Chapter(
                        chapter_id=str(uuid.uuid4()),
                        chapter_number=len(chapters) + 1,
                        title=current_major_title,
                        content=chapter_text,
                        page_range=None
                    ))

            # å¦‚æœæ²¡æœ‰ä¸»è¦ç« èŠ‚ï¼Œåˆ™æŒ‰åŸå§‹åˆ†å‰²
            if not chapters:
                logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°ä¸»è¦ç« èŠ‚ï¼Œä½¿ç”¨åŸå§‹åˆ†å‰²")
                for i, pos in enumerate(chapter_positions):
                    start_line = pos['content_start']
                    if i < len(chapter_positions) - 1:
                        end_line = chapter_positions[i + 1]['line_num']
                    else:
                        end_line = len(lines)

                    chapter_lines = lines[start_line:end_line]
                    chapter_text = '\n'.join(chapter_lines).strip()

                    if chapter_text:
                        chapters.append(Chapter(
                            chapter_id=str(uuid.uuid4()),
                            chapter_number=len(chapters) + 1,
                            title=pos['title'],
                            content=chapter_text,
                            page_range=None
                        ))

        return chapters, stats

    async def _extract_core_viewpoints(
        self,
        chapters: List[Chapter],
        max_viewpoints_per_chapter: int = 5
    ) -> List[CoreViewpoint]:
        """
        æå–æ ¸å¿ƒè§‚ç‚¹

        ç­–ç•¥ï¼š
        1. å¯¹æ¯ä¸ªç« èŠ‚æå–å…³é”®å¥
        2. å¯¹å…³é”®å¥è¿›è¡Œæ€»ç»“
        3. æå–å…³é”®è¯
        """
        core_viewpoints = []

        for chapter in chapters:
            # æå–å…³é”®å¥
            key_sentences = self.text_processor.extract_key_sentences(
                chapter.content,
                top_k=max_viewpoints_per_chapter
            )

            for sentence, score in key_sentences:
                # æå–å…³é”®è¯
                keywords = self.text_processor.extract_keywords(
                    sentence,
                    top_k=5,
                    with_weight=False
                )

                # åˆ›å»ºæ ¸å¿ƒè§‚ç‚¹å¯¹è±¡
                viewpoint = CoreViewpoint(
                    viewpoint_id=str(uuid.uuid4()),
                    content=sentence,
                    original_text=sentence,  # è¿™é‡Œå¯ä»¥ç²¾ç¡®å®šä½åˆ°åŸæ–‡
                    chapter_id=chapter.chapter_id,
                    context=sentence[:100] + "..." if len(sentence) > 100 else sentence,
                    keywords=keywords
                )
                core_viewpoints.append(viewpoint)

        return core_viewpoints


# å…¨å±€å•ä¾‹
_document_parser: Optional[DocumentParser] = None


def get_document_parser() -> DocumentParser:
    """è·å–æ–‡æ¡£è§£æå™¨å•ä¾‹"""
    global _document_parser
    if _document_parser is None:
        _document_parser = DocumentParser()
    return _document_parser


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import asyncio

    async def test():
        """æµ‹è¯•æ–‡æ¡£è§£æ"""
        parser = get_document_parser()

        # æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆä»backendç›®å½•å‡ºå‘ï¼‰
        test_file = "../books/ç†æƒ³å›½.txt"

        try:
            book = await parser.parse_book(
                file_path=test_file,
                title="ç†æƒ³å›½",
                author="æŸæ‹‰å›¾"
            )

            print(f"âœ… è§£ææˆåŠŸ!")
            print(f"æ ‡é¢˜: {book.title}")
            print(f"ä½œè€…: {book.author}")
            print(f"æ€»å­—æ•°: {book.total_words}")
            print(f"ç« èŠ‚æ•°: {len(book.chapters)}")
            print(f"æ ¸å¿ƒè§‚ç‚¹æ•°: {len(book.core_viewpoints)}")

            print("\nç« èŠ‚åˆ—è¡¨:")
            for chapter in book.chapters[:5]:  # åªæ˜¾ç¤ºå‰5ç« 
                print(f"  - {chapter.title}")

            print("\næ ¸å¿ƒè§‚ç‚¹ç¤ºä¾‹:")
            for viewpoint in book.core_viewpoints[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  - {viewpoint.content[:50]}...")

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test())
